[DEFAULT]
workers = 0

[general]
dataset = share-clef
training_subset = train
prediction_subset = dev
nil_symbol = CUI-less
dataset_cache = ${rootpath}/runs/dscache/{}.tgz

[logging]
format = %(asctime)s - %(message)s
level = INFO
summary_fn = ${rootpath}/runs/summaries/${timestamp}.txt
prediction_fn = ${rootpath}/runs/predictions/${timestamp}.tsv
detailed_fn = ${rootpath}/runs/detailed/${timestamp}.{}.tsv
trec_eval_fn = ${rootpath}/runs/trec_eva/${timestamp}.{}.tsv

[candidates]
generator = SGramCosine(.5, 10)
oracle = {"train": 3, "predict": 0}

[emb]
sample_size = 50
context_size = 0
embedding_dim = 50
embedding_voc = 10000
vectorizer_cache = True
tokenizer = whitespace
preprocess = none
embedding_fn = ${rootpath}/data/embeddings/wvec_200_win-30_chiu-et-al.kv
trainable = False

[emb_sub]
sample_size = ${emb:sample_size}
context_size = ${emb:context_size}
embedding_dim = ${emb:embedding_dim}
embedding_voc = ${emb:embedding_voc}
vectorizer_cache = ${emb:vectorizer_cache}
tokenizer = bpe
tokenizer_model = ${rootpath}/data/embeddings/bpe_pmcoa10000model
preprocess = none
embedding_fn = ${rootpath}/data/embeddings/bpe_pmcoa10000_vectors200.txt
trainable = False

[rank]
embeddings = ["emb"]
n_kernels = 50
filter_width = [3]
activation = tanh
optimizer = {"class_name": "adam", "config": {"amsgrad": true, "lr": 1e-4}}
loss = binary_crossentropy
epochs = 100
batch_size = 32
min_score = 0.0

[stop]
min_delta = 0
patience = 3
baseline = 0

[ncbi-disease]
train_fn = ${rootpath}/data/ncbi-disease/NCBItrainset_corpus.txt
dev_fn = ${rootpath}/data/ncbi-disease/NCBIdevelopset_corpus.txt
test_fn = ${rootpath}/data/ncbi-disease/NCBItestset_corpus.txt
dict_fn = ${rootpath}/data/ncbi-disease/CTD_diseases.tsv

[share-clef]
train_fn = ${rootpath}/data/share-clef/train/
dev_fn = ${rootpath}/data/share-clef/dev/
test_fn = ${rootpath}/data/share-clef/test/
dict_fn = ${rootpath}/data/share-clef/SNOMED.tsv

[emb_stem]
sample_size = ${emb:sample_size}
context_size = ${emb:context_size}
embedding_dim = ${emb:embedding_dim}
embedding_voc = ${emb:embedding_voc}
vectorizer_cache = ${emb:vectorizer_cache}
tokenizer = whitespace
preprocess = stem
embedding_fn = ${rootpath}/data/embeddings/wvec_200_win-30_chiu-et-al.kv
trainable = False
